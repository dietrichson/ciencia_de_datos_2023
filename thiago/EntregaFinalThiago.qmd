Este código en R se encarga de realizar una búsqueda de noticias en una fuente específica (en este caso, "pagina12.com.ar") en un rango de fechas definido, recuperar los resultados de la búsqueda desde la API de GDELT, almacenar los datos en un DataFrame, realizar una deduplicación de los resultados y exportarlos a un archivo CSV. Aquí está el desglose del código paso a paso:

Vamos a usar la fuente de Pagina12 para esta entrega.

```{r}
#| eval: false
#########################
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("readr")) install.packages("readr")
if (!require("dplyr")) install.packages("dplyr")
if (!require("tidytext")) install.packages("tidytext")
library(tidyverse)
library(readr)
library(dplyr)
library(tidytext)
library(stringr) # Part of the tidyverse package


read_csv(Headlines,filename)
#Cleanup
rm(Thisfetch,
   dates,
   enddate,
   enddate2,
   filename,
   query,
   sources,
   startdate,
   startdate2,
   thisdate,
   thissource,
   URL_encoded,
   URL_p1,
   URL_p2,
   URL_p3,
   URL_p4,
   URL_p5,
   URL_raw)

#########################
#Headline word counts
WordCounts <- Headlines %>% 
  unnest_tokens(word,Title) %>% 
  count(word, sort = TRUE)
# Deleting standard stop words
data("stop_words")
WordCounts <- WordCounts %>%
  anti_join(stop_words)
# Deleting custom stop words
my_stopwords <- tibble(word = c("and",
                                "the",
                                "etc."))
WordCounts <- WordCounts %>% 
  anti_join(my_stopwords)
rm(stop_words,
   my_stopwords)
```


Luego, contamos qué palabras se repiten en esos contenidos.

```{r}
#| eval: false
# Instala y carga la biblioteca necesaria (si no lo has hecho antes)
##install.packages("tm")
##install.packages("matrixStats")  # Agregar esta línea para corregir el error
library(matrixStats)  # Agregar esta línea para cargar matrixStats
library(tm)
library(tidyverse)
library(tidytext)
library(ggwordcloud)
library(stopwords)
library(topicmodels)
library(ldatuning)
library(syuzhet)

# Cargar el archivo .rds
my_data <- readRDS(here::here("data/scraping-pagina12-2023-10-09.rds"))


my_data %>% 
  filter(stringr::str_detect(content,"narcotráfico")) -> narcotrafico

my_data %>% 
  filter(stringr::str_detect(content,"narco")) -> narco

my_data %>% 
  filter(stringr::str_detect(content,"droga")) -> droga


data <- bind_rows(narcotrafico, narco, droga)

# Accede a la tercera columna "content"
textos <- data$content

# Crear un corpus de texto
corpus <- Corpus(VectorSource(textos))

# Preprocesamiento del texto
corpus <- tm_map(corpus, content_transformer(tolower))  # Convierte a minúsculas
corpus <- tm_map(corpus, removePunctuation)            # Elimina puntuación
corpus <- tm_map(corpus, removeNumbers)               # Elimina números

# Cargar una lista de stopwords en español (o el idioma de tu texto)
stopwords <- stopwords("spanish")

# Eliminar stopwords
corpus <- tm_map(corpus, removeWords, stopwords)


# Crear una matriz de términos (Document-Term Matrix)
dtm <- DocumentTermMatrix(corpus)

# Lista de stopwords personalizadas que deseas eliminar del conteo
stopwords_personalizadas <- c("“", "dos", "años", "dijo", "ser","hace","después","foto")

# Eliminar stopwords personalizadas
dtm <- dtm[, !(colnames(dtm) %in% stopwords_personalizadas)]

# Calcular la suma de frecuencias de cada palabra en todo el corpus
word_freq <- colSums(as.matrix(dtm))

# Ordenar las palabras por frecuencia en orden descendente
word_freq <- sort(word_freq, decreasing = TRUE)

# Mostrar las palabras más frecuentes y sus frecuencias
head(word_freq, 10)


```
